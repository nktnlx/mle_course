{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11202432,"sourceType":"datasetVersion","datasetId":6994395}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformers - Part II","metadata":{}},{"cell_type":"markdown","source":"## Tasks","metadata":{}},{"cell_type":"markdown","source":"### Task 1","metadata":{}},{"cell_type":"markdown","source":"Create embeddings for objects from a subset of the `IMDB` dataset using pre-trained models from Hugging Face.  \nIn this task, make them using the `BERT` model (`bert-base-cased`), using the `get_embeddings_labels function`.  \nCheck before submitting that the tensor with embeddings has a size of (200, 768).  ","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, Subset\nfrom transformers import AutoTokenizer, BertModel, RobertaModel, DistilBertModel\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:03:22.691584Z","iopub.execute_input":"2025-03-30T09:03:22.691832Z","iopub.status.idle":"2025-03-30T09:03:58.230103Z","shell.execute_reply.started":"2025-03-30T09:03:22.691809Z","shell.execute_reply":"2025-03-30T09:03:58.229127Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:03:58.247227Z","iopub.execute_input":"2025-03-30T09:03:58.247655Z","iopub.status.idle":"2025-03-30T09:03:58.318890Z","shell.execute_reply.started":"2025-03-30T09:03:58.247593Z","shell.execute_reply":"2025-03-30T09:03:58.317732Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Function to get model and tokenizer\ndef get_model(model_name):\n    assert model_name in ['bert', 'roberta', 'distilbert']\n    \n    checkpoint_names = {\n        'bert': 'bert-base-cased',\n        'roberta': 'roberta-base',\n        'distilbert': 'distilbert-base-cased'\n    }\n    \n    model_classes = {\n        'bert': BertModel,\n        'roberta': RobertaModel,\n        'distilbert': DistilBertModel\n    }\n    \n    return AutoTokenizer.from_pretrained(checkpoint_names[model_name]), model_classes[model_name].from_pretrained(checkpoint_names[model_name])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:03:58.319914Z","iopub.execute_input":"2025-03-30T09:03:58.320273Z","iopub.status.idle":"2025-03-30T09:03:58.341955Z","shell.execute_reply.started":"2025-03-30T09:03:58.320237Z","shell.execute_reply":"2025-03-30T09:03:58.340697Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Get BERT model and tokenizer\ntokenizer, model = get_model('bert')\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:03:58.342990Z","iopub.execute_input":"2025-03-30T09:03:58.343352Z","iopub.status.idle":"2025-03-30T09:04:12.308530Z","shell.execute_reply.started":"2025-03-30T09:03:58.343317Z","shell.execute_reply":"2025-03-30T09:04:12.307683Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"841eb3f597b642cfb3be5b0d796e6297"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9b33e6c84104b92a48d9cfe1a834100"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"565862df2c7941e98a4bd896b2f13746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07aeee1f890643c5805ab7258e9e4db1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83878a44abf54dc58ddd2436aadf0273"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Function to get embeddings\n@torch.inference_mode()\ndef get_embeddings_labels(model, loader):\n    model.eval()\n    \n    total_embeddings = []\n    labels = []\n    \n    for batch in tqdm(loader):\n        labels.append(batch['labels'].unsqueeze(1))\n\n        batch = {key: batch[key].to(device) for key in ['attention_mask', 'input_ids']}\n\n        embeddings = model(**batch)['last_hidden_state'][:, 0, :]\n\n        total_embeddings.append(embeddings.cpu())\n\n    return torch.cat(total_embeddings, dim=0), torch.cat(labels, dim=0).to(torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:04:12.310307Z","iopub.execute_input":"2025-03-30T09:04:12.310738Z","iopub.status.idle":"2025-03-30T09:04:12.316783Z","shell.execute_reply.started":"2025-03-30T09:04:12.310699Z","shell.execute_reply":"2025-03-30T09:04:12.315649Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load the IMDB dataset\ndataset = load_dataset('imdb', split='train')\n\n# Generate 200 random indices\nnp.random.seed(100)\nidx = np.random.randint(len(dataset), size=200).tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:04:12.317530Z","iopub.execute_input":"2025-03-30T09:04:12.317820Z","iopub.status.idle":"2025-03-30T09:04:17.056463Z","shell.execute_reply.started":"2025-03-30T09:04:12.317795Z","shell.execute_reply":"2025-03-30T09:04:17.055278Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e891da34de774f6c850280a8b193666d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce1cd93bac1a4e1f8d81165cdbee9793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b29fc0f41474abfbe5329a0177d1f7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"987cd973a1144814b958935a88da5c99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8963957ac15415fbaba8dab6bbfd6ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01345b62a2db46a7815f5c9499a2a023"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1133c0d72d44326b32e68f7d166ce05"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Define a collate function for the DataLoader\ndef collate_fn(batch):\n    texts = [item['text'] for item in batch]\n    labels = [item['label'] for item in batch]\n    \n    encoded = tokenizer(\n        texts, \n        padding='max_length', \n        truncation=True, \n        max_length=512, \n        return_tensors='pt'\n    )\n    \n    encoded['labels'] = torch.tensor(labels)\n    \n    return encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:04:21.401818Z","iopub.execute_input":"2025-03-30T09:04:21.402154Z","iopub.status.idle":"2025-03-30T09:04:21.407852Z","shell.execute_reply.started":"2025-03-30T09:04:21.402129Z","shell.execute_reply":"2025-03-30T09:04:21.406689Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Create a subset of the dataset\nsubset = Subset(dataset, idx)\n\n# Create a DataLoader with shuffle=False as required\nloader = DataLoader(subset, batch_size=16, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:04:30.545494Z","iopub.execute_input":"2025-03-30T09:04:30.545909Z","iopub.status.idle":"2025-03-30T09:04:30.551106Z","shell.execute_reply.started":"2025-03-30T09:04:30.545879Z","shell.execute_reply":"2025-03-30T09:04:30.549942Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Get embeddings and labels\nembeddings, labels = get_embeddings_labels(model, loader)\n\n# Check the shape of the embeddings\nembedding_shape = embeddings.shape\nprint(f'Embeddings shape: {embedding_shape}')\n\n# Save the embeddings to a file\ntorch.save(embeddings, 'bert_embeddings.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:04:38.640720Z","iopub.execute_input":"2025-03-30T09:04:38.641062Z","iopub.status.idle":"2025-03-30T09:07:42.630193Z","shell.execute_reply.started":"2025-03-30T09:04:38.641039Z","shell.execute_reply":"2025-03-30T09:07:42.628768Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 13/13 [03:03<00:00, 14.15s/it]","output_type":"stream"},{"name":"stdout","text":"Embeddings shape: torch.Size([200, 768])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### Task 2","metadata":{}},{"cell_type":"markdown","source":"Create embeddings for objects from a subset of the `IMDB` dataset using pre-trained models from Hugging Face.  \nIn this task, make them using the `RoBERTa` model (`roberta-base`), using the `get_embeddings_labels function`.  \nCheck before submitting that the tensor with embeddings has a size of (200, 768).  ","metadata":{}},{"cell_type":"code","source":"tokenizer, model = get_model('roberta')\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:11:22.415641Z","iopub.execute_input":"2025-03-30T09:11:22.416093Z","iopub.status.idle":"2025-03-30T09:11:25.859680Z","shell.execute_reply.started":"2025-03-30T09:11:22.416059Z","shell.execute_reply":"2025-03-30T09:11:25.858703Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea9a81a39e9e4907a9272a3496a5ab73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ccb0a3423143d4883b518daf60b4bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001884eb7774413894f56ea68b824aef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e452d0963f94ffa9483fcfbd7ae4e8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec396f420600462587db4decdc878ee9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f40640464e14c3386e3cad8d9330d11"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Get embeddings and labels\nembeddings, labels = get_embeddings_labels(model, loader)\n\n# Check the shape of the embeddings\nembedding_shape = embeddings.shape\nprint(f'Embeddings shape: {embedding_shape}')\n\n# Save the embeddings to a file\ntorch.save(embeddings, 'roberta_embeddings.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:11:38.093656Z","iopub.execute_input":"2025-03-30T09:11:38.094029Z","iopub.status.idle":"2025-03-30T09:14:38.414981Z","shell.execute_reply.started":"2025-03-30T09:11:38.094000Z","shell.execute_reply":"2025-03-30T09:14:38.413897Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 13/13 [03:00<00:00, 13.87s/it]","output_type":"stream"},{"name":"stdout","text":"Embeddings shape: torch.Size([200, 768])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### Task 3","metadata":{}},{"cell_type":"markdown","source":"Create embeddings for objects from a subset of the `IMDB` dataset using pre-trained models from Hugging Face.  \nIn this task, make them using the `DistilBERT` model (`distilbert-base-cased`), using the `get_embeddings_labels function`.  \nCheck before submitting that the tensor with embeddings has a size of (200, 768).  ","metadata":{}},{"cell_type":"code","source":"tokenizer, model = get_model('distilbert')\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:15:59.119694Z","iopub.execute_input":"2025-03-30T09:15:59.120102Z","iopub.status.idle":"2025-03-30T09:16:01.262967Z","shell.execute_reply.started":"2025-03-30T09:15:59.120072Z","shell.execute_reply":"2025-03-30T09:16:01.262011Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b3036cccf614b7ea4500147fff623ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42d33cc172024e8ebf65f62ee578b099"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f00555b911e42c8a4c7031e81393742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7184c51737c4d95bff41b522289103d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/263M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0691acd1e0494ceda7310ad15dc51744"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Get embeddings and labels\nembeddings, labels = get_embeddings_labels(model, loader)\n\n# Check the shape of the embeddings\nembedding_shape = embeddings.shape\nprint(f'Embeddings shape: {embedding_shape}')\n\n# Save the embeddings to a file\ntorch.save(embeddings, 'distilbert_embeddings.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:16:10.799979Z","iopub.execute_input":"2025-03-30T09:16:10.800315Z","iopub.status.idle":"2025-03-30T09:17:39.908872Z","shell.execute_reply.started":"2025-03-30T09:16:10.800290Z","shell.execute_reply":"2025-03-30T09:17:39.907690Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 13/13 [01:29<00:00,  6.85s/it]","output_type":"stream"},{"name":"stdout","text":"Embeddings shape: torch.Size([200, 768])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}