{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11083072,"sourceType":"datasetVersion","datasetId":6907747}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Objects Detection","metadata":{}},{"cell_type":"markdown","source":"## Tasks","metadata":{}},{"cell_type":"markdown","source":"### Task 1","metadata":{}},{"cell_type":"markdown","source":"Calculate the `Jaccard similarity` for class 0, class 1, and class 2.  \nCalculate the average and round to 4 decimal places.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import Tensor\n\n\ny_pred = Tensor([\n    [1, 1, 2, 2, 2],\n    [1, 1, 2, 1, 2],\n    [1, 0, 0, 0, 0],\n    [2, 2, 2, 0, 0],\n    [2, 1, 1, 1, 2]\n])\n\ny_true = Tensor([\n    [1, 1, 1, 2, 2],\n    [1, 1, 1, 2, 2],\n    [1, 1, 1, 2, 2],\n    [0, 0, 0, 2, 2],\n    [0, 0, 0, 2, 2]\n])\n\n# Flatten the tensors\ny_pred_flat = y_pred.flatten()\ny_true_flat = y_true.flatten()\n\n# Calculate Jaccard similarity for class 0\npred_class_0 = (y_pred_flat == 0)\ntrue_class_0 = (y_true_flat == 0)\nintersection_0 = (pred_class_0 & true_class_0).sum().item()\nunion_0 = (pred_class_0 | true_class_0).sum().item()\njaccard_0 = intersection_0 / union_0 if union_0 > 0 else 0\n\n# Calculate Jaccard similarity for class 1\npred_class_1 = (y_pred_flat == 1)\ntrue_class_1 = (y_true_flat == 1)\nintersection_1 = (pred_class_1 & true_class_1).sum().item()\nunion_1 = (pred_class_1 | true_class_1).sum().item()\njaccard_1 = intersection_1 / union_1 if union_1 > 0 else 0\n\n# Calculate Jaccard similarity for class 2\npred_class_2 = (y_pred_flat == 2)\ntrue_class_2 = (y_true_flat == 2)\nintersection_2 = (pred_class_2 & true_class_2).sum().item()\nunion_2 = (pred_class_2 | true_class_2).sum().item()\njaccard_2 = intersection_2 / union_2 if union_2 > 0 else 0\n\n# Calculate the average\naverage_jaccard = (jaccard_0 + jaccard_1 + jaccard_2) / 3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T05:13:59.999943Z","iopub.execute_input":"2025-03-19T05:14:00.000393Z","iopub.status.idle":"2025-03-19T05:14:01.945352Z","shell.execute_reply.started":"2025-03-19T05:14:00.000352Z","shell.execute_reply":"2025-03-19T05:14:01.944180Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"print(f'Class 0 - Intersection: {intersection_0}, Union: {union_0}, Jaccard: {jaccard_0:.4f}')\nprint(f'Class 1 - Intersection: {intersection_1}, Union: {union_1}, Jaccard: {jaccard_1:.4f}')\nprint(f'Class 2 - Intersection: {intersection_2}, Union: {union_2}, Jaccard: {jaccard_2:.4f}')\nprint(f'Average Jaccard similarity: {average_jaccard:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T05:14:03.999677Z","iopub.execute_input":"2025-03-19T05:14:04.000215Z","iopub.status.idle":"2025-03-19T05:14:04.007700Z","shell.execute_reply.started":"2025-03-19T05:14:04.000177Z","shell.execute_reply":"2025-03-19T05:14:04.006681Z"}},"outputs":[{"name":"stdout","text":"Class 0 - Intersection: 0, Union: 12, Jaccard: 0.0000\nClass 1 - Intersection: 5, Union: 13, Jaccard: 0.3846\nClass 2 - Intersection: 4, Union: 16, Jaccard: 0.2500\nAverage Jaccard similarity: 0.2115\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Task 2","metadata":{}},{"cell_type":"markdown","source":"Enhance the `UNET` model from the seminar, train it on the `OXFORD-PETS` dataset, and achieve a pixel-wise `Accuracy` of 88%. To do this, you'll need to add more downsampling blocks and upsampling blocks, and possibly increase the base_channels.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\n\ndef conv_plus_conv(in_channels: int, out_channels: int):\n    return nn.Sequential(\n        nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        ),\n        nn.BatchNorm2d(num_features=out_channels),\n        nn.LeakyReLU(0.2),\n        nn.Conv2d(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        ),\n        nn.BatchNorm2d(num_features=out_channels),\n        nn.LeakyReLU(0.2),\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UNET(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        base_channels = 32\n\n        self.down1 = conv_plus_conv(3, base_channels)\n        self.down2 = conv_plus_conv(base_channels, base_channels * 2)\n        self.down3 = conv_plus_conv(base_channels * 2, base_channels * 4)\n        self.down4 = conv_plus_conv(base_channels * 4, base_channels * 8)\n        self.down5 = conv_plus_conv(base_channels * 8, base_channels * 16)\n\n        self.up1 = conv_plus_conv(base_channels * 2, base_channels)\n        self.up2 = conv_plus_conv(base_channels * 4, base_channels)\n        self.up3 = conv_plus_conv(base_channels * 8, base_channels * 2)\n        self.up4 = conv_plus_conv(base_channels * 16, base_channels * 4)\n        self.up5 = conv_plus_conv(base_channels * 32, base_channels * 8)\n\n        self.bottleneck = conv_plus_conv(base_channels * 16, base_channels * 16)\n\n        self.out = nn.Conv2d(in_channels=base_channels, out_channels=3, kernel_size=1)\n\n        self.downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    \n    def forward(self, x):\n        # x.shape = (N, N, 3)\n\n        residual1 = self.down1(x)  # x.shape: (N, N, 3) -> (N, N, base_channels)\n        x = self.downsample(residual1)  # x.shape: (N, N, base_channels) -> (N // 2, N // 2, base_channels)\n\n        residual2 = self.down2(x)  # x.shape: (N // 2, N // 2, base_channels) -> (N // 2, N // 2, base_channels * 2)\n        x = self.downsample(residual2)  # x.shape: (N // 2, N // 2, base_channels * 2) -> (N // 4, N // 4, base_channels * 2)\n\n        residual3 = self.down3(x)\n        x = self.downsample(residual3)\n\n        residual4 = self.down4(x)\n        x = self.downsample(residual4)\n\n        residual5 = self.down5(x)\n        x = self.downsample(residual5)\n\n        # LATENT SPACE DIMENSION DIM = N // 4\n        x = self.bottleneck(x)  # x.shape: (N // 4, N // 4, base_channels * 2) -> (N // 4, N // 4, base_channels * 2)\n\n        x = nn.functional.interpolate(x, scale_factor=2)\n        x = torch.cat((x, residual5), dim=1)\n        x = self.up5(x)\n\n        x = nn.functional.interpolate(x, scale_factor=2)\n        x = torch.cat((x, residual4), dim=1)\n        x = self.up4(x)\n\n        x = nn.functional.interpolate(x, scale_factor=2)\n        x = torch.cat((x, residual3), dim=1)\n        x = self.up3(x)\n\n        x = nn.functional.interpolate(x, scale_factor=2)  # x.shape: (N // 4, N // 4, base_channels * 2) -> (N // 2, N // 2, base_channels * 2)\n        x = torch.cat((x, residual2), dim=1)  # x.shape: (N // 2, N // 2, base_channels * 2) -> (N // 2, N // 2, base_channels * 4)\n        x = self.up2(x)  # x.shape: (N // 2, N // 2, base_channels * 4) -> (N // 2, N // 2, base_channels)\n\n        x = nn.functional.interpolate(x, scale_factor=2)  # x.shape: (N // 2, N // 2, base_channels) -> (N, N, base_channels)\n        x = torch.cat((x, residual1), dim=1)  # x.shape: (N, N, base_channels) -> (N, N, base_channels * 2)\n        x = self.up1(x)  # x.shape: (N, N, base_channels * 2) -> (N, N, base_channels)\n\n        x = self.out(x)  # x.shape: (N, N, base_channels) -> (N, N, 3)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\nfrom IPython.display import clear_output\nfrom torch.optim import Adam\nfrom torch.optim import Optimizer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Subset\nfrom torchvision.datasets import OxfordIIITPet\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed):\n    torch.backends.cudnn.deterministic = True\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(\n    model: nn.Module,\n    data_loader: DataLoader,\n    optimizer: Optimizer,\n    loss_fn,\n    device: torch.device,\n):\n    model.train()\n\n    train_loss = 0\n    total = 0\n    correct = 0\n\n    for x, y in tqdm(data_loader, desc='Train'):\n        bs = y.size(0)\n\n        x, y = x.to(device), y.squeeze(1).to(device)\n\n        optimizer.zero_grad()\n\n        output = model(x)\n\n        loss = loss_fn(output.reshape(bs, 3, -1), y.reshape(bs, -1))\n\n        train_loss += loss.item()\n\n        loss.backward()\n\n        optimizer.step()\n\n        _, y_pred = output.max(dim=1)\n        total += y.size(0) * y.size(1) * y.size(2)\n        correct += (y == y_pred).sum().item()\n\n    train_loss /= len(data_loader)\n    accuracy = correct / total\n\n    return train_loss, accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.inference_mode()\ndef evaluate(\n    model: nn.Module, data_loader: DataLoader, loss_fn, device: torch.device\n):\n    model.eval()\n\n    total_loss = 0\n    total = 0\n    correct = 0\n\n    for x, y in tqdm(data_loader, desc='Evaluation'):\n        bs = y.size(0)\n\n        x, y = x.to(device), y.squeeze(1).to(device)\n\n        output = model(x)\n\n        loss = loss_fn(output.reshape(bs, 3, -1), y.reshape(bs, -1))\n\n        total_loss += loss.item()\n\n        _, y_pred = output.max(dim=1)\n        total += y.size(0) * y.size(1) * y.size(2)\n        correct += (y == y_pred).sum().item()\n\n    total_loss /= len(data_loader)\n    accuracy = correct / total\n\n    return total_loss, accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_stats(\n    train_loss: list[float],\n    valid_loss: list[float],\n    train_accuracy: list[float],\n    valid_accuracy: list[float],\n    title: str\n):\n    plt.figure(figsize=(16, 8))\n\n    plt.title(title + ' loss')\n\n    plt.plot(train_loss, label='Train loss')\n    plt.plot(valid_loss, label='Valid loss')\n    plt.legend()\n    plt.grid()\n\n    plt.show()\n\n    plt.figure(figsize=(16, 8))\n\n    plt.title(title + ' accuracy')\n    \n    plt.plot(train_accuracy, label='Train accuracy')\n    plt.plot(valid_accuracy, label='Valid accuracy')\n    plt.legend()\n    plt.grid()\n\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def whole_train_valid_cycle(\n    model, train_loader, valid_loader, optimizer, loss_fn, device, threshold, title\n):\n    train_loss_history, valid_loss_history = [], []\n    train_accuracy_history, valid_accuracy_history = [], []\n\n    for epoch in range(100):\n        train_loss, train_accuracy = train(\n            model, train_loader, optimizer, loss_fn, device\n        )\n        valid_loss, valid_accuracy = evaluate(model, valid_loader, loss_fn, device)\n\n        train_loss_history.append(train_loss)\n        valid_loss_history.append(valid_loss)\n\n        train_accuracy_history.append(train_accuracy)\n        valid_accuracy_history.append(valid_accuracy)\n\n        clear_output(wait=True)\n\n        plot_stats(\n            train_loss_history,\n            valid_loss_history,\n            train_accuracy_history,\n            valid_accuracy_history,\n            title,\n        )\n\n        if valid_accuracy >= threshold:\n            break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.inference_mode()\ndef predict_segmentation(model: nn.Module, loader: DataLoader, device: torch.device):\n    model.eval()\n\n    prediction = []\n\n    for x, _ in loader:\n        output = model(x.to(device)).cpu()\n\n        prediction.append(torch.argmax(output, dim=1))\n\n    prediction = torch.cat(prediction)\n\n    return prediction","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main(model_class, threshold, title):\n    set_seed(0xDEADF00D)\n\n    transform = T.Compose(\n        [\n            T.Resize((256, 256)),\n            T.ToTensor(),\n        ]\n    )\n\n    target_transform = T.Compose(\n        [\n            T.Resize((256, 256)),\n            T.PILToTensor(),\n            T.Lambda(lambda x: (x - 1).long())\n        ]\n    )\n\n    train_dataset = OxfordIIITPet('/home/jupyter/mnt/datasets/pets', transform=transform, download=True, target_transform=target_transform, target_types='segmentation')\n    valid_dataset = OxfordIIITPet('/home/jupyter/mnt/datasets/pets', transform=transform, download=True, split='test', target_transform=target_transform, target_types='segmentation')\n    \n    np.random.seed(100)\n    idx = np.random.randint(len(valid_dataset), size=200).tolist()\n    \n    valid_dataset = Subset(valid_dataset, idx)\n\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)\n\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    model = model_class().to(device)\n\n    optimizer = Adam(model.parameters(), lr=1e-3)\n\n    loss_fn = nn.CrossEntropyLoss()\n\n    whole_train_valid_cycle(\n        model, train_loader, valid_loader, optimizer, loss_fn, device, threshold, title\n    )\n\n    torch.save(predict_segmentation(model, valid_loader, device).reshape([200, 1, 256, 256]).to(torch.uint8), 'prediction.pt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main(UNET, 0.88, 'UNET segmentation')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}